{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 — Text-Driven Image Segmentation with SAM 2 (Colab)\n",
    "\n",
    "This notebook demonstrates a pipeline: Image -> Text prompt -> Convert prompt to region seeds (GroundingDINO) -> feed seeds to SAM -> display mask overlay.\n",
    "\n",
    "Run in Colab and provide model checkpoints in the runtime when prompted. Top cells install dependencies; run them first in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab helper: install required packages when running in Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in Colab: if needed, uncomment install lines below and run this cell to install dependencies (may take several minutes)')\n",
    "    # Uncomment the following lines in Colab to install\n",
    "    # !pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
    "    # !pip install -q git+https://github.com/IDEA-Research/GroundingDINO.git\n",
    "    # !pip install -q transformers timm opencv-python-headless matplotlib\n",
    "else:\n",
    "    print('Not running in Colab; skip installs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (works in local env if packages are installed; in Colab run the install cell first)\n",
    "import torch, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# The heavy imports are wrapped so the notebook can be opened without installing packages\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    from groundingdino.util.inference import load_model, load_image, predict_with_caption\n",
    "except Exception as e:\n",
    "    print('Optional imports failed (expected if not installed). Install the packages in Colab before running the full pipeline:', e)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device ->', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Provide or download an image\n",
    "\n",
    "You can either upload an image via Colab's UI or download a sample image using the cell below. Replace the URL with your image if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: download a sample image (replace the URL or use upload)\n",
    "import urllib.request\n",
    "sample_url = 'https://images.unsplash.com/photo-1518791841217-8f162f1e1131'\n",
    "img_path = 'sample.jpg'\n",
    "urllib.request.urlretrieve(sample_url, img_path)\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(6,6)); plt.imshow(img_rgb); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Set model checkpoint paths (download in Colab)\n",
    "\n",
    "You must download GroundingDINO and SAM checkpoints into the Colab runtime and set the paths below. Links are in the respective repos. For GroundingDINO use a Swin transformer checkpoint; for SAM, use a ViT checkpoint (sam_vit_h.pth or sam_vit_b.pth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these paths after you download the weights to the Colab runtime\n",
    "GROUNDING_DINO_WEIGHTS = '/content/groundingdino_swint_ogc.pth'  # <- download and set in Colab\n",
    "GROUNDING_DINO_CONFIG = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'  # path inside cloned repo\n",
    "SAM_WEIGHTS = '/content/sam_vit_h_4b8939.pth'  # <- download and set in Colab\n",
    "print('Set the model weight paths and ensure files exist in Colab before running detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GroundingDINO: convert text prompt to boxes/phrases\n",
    "\n",
    "The cell below runs GroundingDINO inference to return bounding boxes and phrases for the given text prompt. Boxes are in [x0,y0,x1,y1] (pixel coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a red bicycle'  # replace with your prompt\n",
    "print('Prompt:', prompt)\n",
    "boxes, logits, phrases = None, None, None\n",
    "try:\n",
    "    gd_model = load_model(GROUNDING_DINO_CONFIG, GROUNDING_DINO_WEIGHTS, device=device)\n",
    "    boxes, logits, phrases = predict_with_caption(gd_model, img_path, prompt)\n",
    "    print('Found phrases:', phrases)\n",
    "    print('Boxes count:', 0 if boxes is None else len(boxes))\n",
    "except Exception as e:\n",
    "    print('GroundingDINO inference failed; ensure weights and config are correct and installed in Colab:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run SAM using the detected boxes as prompts\n",
    "\n",
    "This section loads SAM and feeds the boxes as prompts. SAM will produce masks which we overlay on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sam = sam_model_registry['vit_h'](checkpoint=SAM_WEIGHTS).to(device)\n",
    "    predictor = SamPredictor(sam)\n",
    "    predictor.set_image(img_rgb)\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        boxes_xyxy = boxes  # GroundingDINO boxes are [x0,y0,x1,y1]\n",
    "        import torch as _t\n",
    "        transformed_boxes = predictor.transform.apply_boxes_torch(_t.tensor(boxes_xyxy).to(device), img_rgb.shape[:2])\n",
    "        masks, scores, logits = predictor.predict_torch(boxes=transformed_boxes, multimask_output=False)\n",
    "        masks = masks.cpu().numpy()\n",
    "        # Display masks overlayed\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(img_rgb); plt.axis('off')\n",
    "        for m in masks:\n",
    "            plt.imshow(np.ma.masked_where(m==0, m), alpha=0.5)\n",
    "        plt.title('SAM masks (from text prompt)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No boxes found to feed to SAM. Try a different prompt or check GroundingDINO output.')\n",
    "except Exception as e:\n",
    "    print('SAM inference failed. Ensure segment-anything is installed and SAM_WEIGHTS path is correct:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and limitations\n",
    "- You must download pretrained weights for GroundingDINO and SAM into Colab runtime; links are available in the respective GitHub repos.\n",
    "- This notebook uses GroundingDINO to convert text prompts to bounding boxes. Alternatives: CLIPSeg, GLIP, or fine-tuned text->box models.\n",
    "- If detection fails for ambiguous prompts, try more specific phrases or use multiple prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 — Text-Driven Image Segmentation with SAM 2 (Colab)\n",
    "\n",
    "This notebook demonstrates a pipeline: Image -> Text prompt -> Convert prompt to region seeds (GroundingDINO) -> feed seeds to SAM -> display mask overlay.\n",
    "\n",
    "Run in Colab and provide model checkpoints in the runtime when prompted. Top cells install dependencies; run them first in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies in Colab (uncomment when running in Colab)\n",
    "# !pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
    "# !pip install -q git+https://github.com/IDEA-Research/GroundingDINO.git\n",
    "# !pip install -q transformers timm opencv-python-headless matplotlib\n",
    "# Note: You will need to download pretrained weights for GroundingDINO and SAM in Colab and set the paths below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# The segment-anything and groundingdino imports are left inside try/except so the notebook can be inspected without running installs\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# The segment-anything and groundingdino imports are left inside try/except so the notebook can be inspected without running installs\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    from groundingdino.util.inference import load_model, load_image, predict_with_caption\n",
    "except Exception as e:\n",
    "    print('Optional imports failed (expected if not installed). Install the packages in Colab before running the full pipeline:', e)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Provide or download an image\n",
    "You can either upload an image via Colab's UI or download a sample image using the cell below. Replace the URL with your image if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: download a sample image (replace the URL or use upload)\n",
    "import urllib.request\n",
    "sample_url = 'https://images.unsplash.com/photo-1518791841217-8f162f1e1131'\n",
    "img_path = 'sample.jpg'\n",
    "urllib.request.urlretrieve(sample_url, img_path)\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(6,6)); plt.imshow(img_rgb); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Set model checkpoint paths (download in Colab)\n",
    "\n",
    "You must download GroundingDINO and SAM checkpoints into the Colab runtime and set the paths below. Links are in the respective repos. For GroundingDINO use a Swin transformer checkpoint; for SAM, use a ViT checkpoint (sam_vit_h.pth or sam_vit_b.pth).\n",
    "If you don't have checkpoints, the notebook will print an instruction message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these paths after you download the weights to the Colab runtime\n",
    "GROUNDING_DINO_WEIGHTS = '/content/groundingdino_swint_ogc.pth'  # <- download and set in Colab\n",
    "GROUNDING_DINO_CONFIG = '/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'  # path inside cloned repo\n",
    "SAM_WEIGHTS = '/content/sam_vit_h_4b8939.pth'  # <- download and set in Colab\n",
    "print('Set the model weight paths and ensure files exist in Colab before running detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GroundingDINO: convert text prompt to boxes/phrases\n",
    "The cell below runs GroundingDINO inference to return bounding boxes and phrases for the given text prompt. Boxes are in [x0,y0,x1,y1] (pixel coordinates). If you prefer CLIP-based alternatives, you can swap in a different detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a red bicycle'  # replace with your prompt\n",
    "print('Prompt:', prompt)\n",
    "boxes, logits, phrases = None, None, None\n",
    "try:\n",
    "    gd_model = load_model(GROUNDING_DINO_CONFIG, GROUNDING_DINO_WEIGHTS, device=device)\n",
    "    boxes, logits, phrases = predict_with_caption(gd_model, img_path, prompt)\n",
    "    print('Found phrases:', phrases)\n",
    "    print('Boxes shape:', None if boxes is None else len(boxes))\n",
    "except Exception as e:\n",
    "    print('GroundingDINO inference failed; ensure weights and config are correct and installed in Colab:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run SAM using the detected boxes as prompts\n",
    "This section loads SAM and feeds the boxes as prompts. SAM will produce masks which we overlay on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sam = sam_model_registry['vit_h'](checkpoint=SAM_WEIGHTS).to(device)\n",
    "    predictor = SamPredictor(sam)\n",
    "    predictor.set_image(img_rgb)\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        boxes_xyxy = boxes  # GroundingDINO boxes are [x0,y0,x1,y1]\n",
    "        import torch as _t\n",
    "        transformed_boxes = predictor.transform.apply_boxes_torch(_t.tensor(boxes_xyxy).to(device), img_rgb.shape[:2])\n",
    "        masks, scores, logits = predictor.predict_torch(boxes=transformed_boxes, multimask_output=False)\n",
    "        masks = masks.cpu().numpy()\n",
    "        # Display masks overlayed\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(img_rgb); plt.axis('off')\n",
    "        for m in masks:\n",
    "            plt.imshow(np.ma.masked_where(m==0, m), alpha=0.5)\n",
    "        plt.title('SAM masks (from text prompt)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No boxes found to feed to SAM. Try a different prompt or check GroundingDINO output.')\n",
    "except Exception as e:\n",
    "    print('SAM inference failed. Ensure segment-anything is installed and SAM_WEIGHTS path is correct:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and limitations\n",
    "- You must download pretrained weights for GroundingDINO and SAM into Colab runtime; links are available in the respective GitHub repos.\n",
    "- This notebook uses GroundingDINO to convert text prompts to bounding boxes. Alternatives: CLIPSeg, GLIP, or fine-tuned text->box models.\n",
    "- If detection fails for ambiguous prompts, try more specific phrases or use multiple prompts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
